from __future__ import annotations

import json
import logging
import uuid
from io import BytesIO, StringIO
from typing import TYPE_CHECKING, Any, Callable, Literal, Optional, Protocol, Type

try:
    from typing import TypedDict  # Python 3.11+
except ImportError:
    from typing_extensions import TypedDict  # Python 3.8â€“3.10

import jmespath
import pandas as pd
from airflow.hooks.base import BaseHook
from airflow.models import BaseOperator

try:
    from airflow.providers.http.operators.http import HttpOperator
except ImportError:
    from airflow.providers.http.operators.http import SimpleHttpOperator as HttpOperator

from airflow.utils.context import Context
from airflow.utils.helpers import merge_dicts
from requests import Response

from airflow_toolkit.compression_utils import CompressionOptions, compress
from airflow_toolkit.exceptions import ApiResponseTypeError
from airflow_toolkit.filesystems.filesystem_factory import FilesystemFactory

if TYPE_CHECKING:
    from requests.auth import AuthBase

SaveFormat = Literal["jsonl"]


class Transformation(Protocol):
    def __call__(self, data: bytes, **kwargs) -> bytes: ...


class HttpBatchOperator(HttpOperator):
    def execute(
        self, context: Context, use_new_data_parameters_on_pagination=False
    ) -> Any:
        self.log.info("Calling HTTP method")

        response = self.hook.run(
            self.endpoint, self.data, self.headers, self.extra_options
        )
        yield self.process_response(context=context, response=response)
        for response in self.paginate_sync(
            response=response,
            use_new_data_parameters_on_pagination=use_new_data_parameters_on_pagination,
        ):
            yield self.process_response(context=context, response=response)

    def paginate_sync(
        self, response: Response, use_new_data_parameters_on_pagination=False
    ) -> Response | list[Response]:
        if not self.pagination_function:
            return None

        while True:
            next_page_params = self.pagination_function(response)
            if not next_page_params:
                break
            response = self.hook.run(
                **self._merge_next_page_parameters(
                    next_page_params, use_new_data_parameters_on_pagination
                )
            )
            yield response
        return None

    def _merge_next_page_parameters(
        self, next_page_params: dict, use_new_data_parameters_on_pagination=False
    ) -> dict:
        """Merge initial request parameters with next page parameters.

        Merge initial requests parameters with the ones for the next page, generated by
        the pagination function. Items in the 'next_page_params' overrides those defined
        in the previous request.

        :param next_page_params: A dictionary containing the parameters for the next page.
        :return: A dictionary containing the merged parameters.
        """
        data: str | dict | None = None  # makes mypy happy
        next_page_data_param = next_page_params.get("data")
        if use_new_data_parameters_on_pagination and isinstance(
            next_page_data_param, dict
        ):
            data = next_page_data_param
        elif isinstance(self.data, dict) and isinstance(next_page_data_param, dict):
            data = merge_dicts(self.data, next_page_data_param)
        else:
            data = next_page_data_param or self.data

        self.log.info(
            f"Calling HTTP method with endpoint: {next_page_params.get('endpoint') or self.endpoint}"
        )
        self.log.info(f"Calling HTTP method with data: {data}")

        return dict(
            endpoint=next_page_params.get("endpoint") or self.endpoint,
            data=data,
            headers=merge_dicts(self.headers, next_page_params.get("headers", {})),
            extra_options=merge_dicts(
                self.extra_options, next_page_params.get("extra_options", {})
            ),
        )


class HttpToFilesystem(BaseOperator):
    template_fields = list(HttpOperator.template_fields) + [
        "filesystem_path",
        "filesystem_conn_id",
        "jmespath_expression",
        "save_format",
        "source_format",
    ]
    template_fields_renderers = HttpOperator.template_fields_renderers

    json_response_source_format = ["json", "jsonl"]
    binary_response_source_format = ["parquet"]

    def __init__(
        self,
        http_conn_id: str,
        filesystem_conn_id: str,
        filesystem_path: str,
        save_format: SaveFormat = "jsonl",
        source_format: SaveFormat = None,
        compression: CompressionOptions = None,
        endpoint: str | None = None,
        method: str = "POST",
        data: Any = None,
        headers: dict[str, str] | None = None,
        auth_type: type["AuthBase"] | None = None,
        jmespath_expression: str | None = None,
        pagination_function: Callable | None = None,
        use_new_data_parameters_on_pagination: bool = False,
        create_file_on_success: str | None = None,
        data_transformation: Optional[Transformation] = None,
        data_transformation_kwargs: dict[str, Any] | None = None,
        file_number_start: int = 1,
        strict_response_schema=True,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        self.http_conn_id = http_conn_id
        self.filesystem_conn_id = filesystem_conn_id
        self.filesystem_path = filesystem_path
        self.compression = compression
        self.endpoint = endpoint
        self.method = method
        self.data = data
        self.headers = headers
        self.auth_type = auth_type
        self.jmespath_expression = jmespath_expression
        self.pagination_function = pagination_function
        self.use_new_data_parameters_on_pagination = (
            use_new_data_parameters_on_pagination
        )
        self.create_file_on_success = create_file_on_success
        self.data_transformation = data_transformation
        self.data_transformation_kwargs = data_transformation_kwargs

        self.save_format = save_format
        self.source_format = source_format if source_format else save_format
        self.file_number_start = file_number_start
        self.strict_response_schema = strict_response_schema
        self.kwargs = kwargs

        if (
            self.save_format in self.binary_response_source_format
            and self.compression is not None
        ):
            raise ValueError(
                f"Compression is not supported for binary response save formats: {self.binary_response_source_format}"
            )

        if self.data_transformation and not callable(self.data_transformation):
            raise ValueError("data_transformation must be a callable")

        if self.data_transformation is None and self.source_format != self.save_format:
            raise ValueError(
                "data_transformation must be provided if source_format is different from save_format"
            )
        if self.data_transformation_kwargs and self.data_transformation is None:
            raise ValueError(
                "data_transformation must be provided if data_transformation_kwargs is provided"
            )

    def execute(self, context: "Context") -> Any:
        http_batch_operator = HttpBatchOperator(
            task_id=f"http-operator-{uuid.uuid4()}",
            http_conn_id=self.http_conn_id,
            endpoint=self.endpoint,
            method=self.method,
            data=self.data,
            headers=self.headers,
            auth_type=self.auth_type,
            response_filter=self._response_filter,
            pagination_function=self.pagination_function,
        )
        for i, data in enumerate(
            http_batch_operator.execute(
                context,
                use_new_data_parameters_on_pagination=self.use_new_data_parameters_on_pagination,
            ),
            start=self.file_number_start,
        ):
            if not self.strict_response_schema and not data:
                logging.info(
                    "No data returned from the API or response filter. Skipping this batch"
                )
                continue
            filesystem_protocol = FilesystemFactory.get_data_lake_filesystem(
                connection=BaseHook.get_connection(self.filesystem_conn_id),
            )

            file_path = self.filesystem_path.rstrip("/") + "/" + self._file_name(i)

            filesystem_protocol.write(data, file_path)

            if self.create_file_on_success is not None and isinstance(
                self.create_file_on_success, str
            ):
                success_file_path = (
                    self.filesystem_path.rstrip("/") + "/" + self.create_file_on_success
                )
                filesystem_protocol.write(BytesIO(), success_file_path)

    def _file_name(self, n_part) -> str:
        file_name = f"part{n_part:04}.{self.save_format}"

        if self.compression:
            file_name += f".{self.compression}"
        return file_name

    def _response_filter(self, response) -> BytesIO:
        if (
            self.jmespath_expression
            and self.source_format in self.json_response_source_format
        ):
            data = jmespath.search(self.jmespath_expression, response.json())

        elif (
            self.jmespath_expression
            and self.source_format not in self.json_response_source_format
        ):
            raise ApiResponseTypeError(
                "JMESPath expression is only supported for json and jsonl save formats"
            )
        elif self.source_format in self.json_response_source_format:
            data = response.json()

        elif self.source_format in self.binary_response_source_format:
            data = response.content
        else:
            data = response.text

        self.response_filter_data = data

        # Check if we have a custom data transformation
        if self.data_transformation and self.data_transformation_kwargs:
            return self.data_transformation(data, self.data_transformation_kwargs)
        elif self.data_transformation:
            return self.data_transformation(data)

        # If we don't have a custom data transformation, use the default one based on the source_format

        match self.source_format:
            case "json":
                return json_to_binary(data, self.compression)

            case "jsonl":
                if self.strict_response_schema and not isinstance(data, list):
                    raise ApiResponseTypeError(
                        "Expected response can't be transformed to jsonl. It is not  list[dict]"
                    )
                elif not isinstance(data, list):
                    logging.warning(
                        "Expected response can't be transformed to jsonl. It is not  list[dict]"
                    )
                    return None
                return list_to_jsonl(data, self.compression)

            case "xml":
                return xml_to_binary(data, self.compression)

            case "parquet":
                return data

            case "csv":
                return csv_to_binary(data, self.compression)

            case _:
                raise NotImplementedError(
                    f"Unknown source_format/save_format: {self.source_format}"
                )


class RequestSpec(TypedDict, total=False):
    """User-provided per-request overrides (all keys optional)."""

    endpoint: str
    method: str
    data: Any
    headers: dict[str, str]
    auth_type: Type["AuthBase"] | None
    jmespath_expression: str | None
    save_format: "SaveFormat"
    source_format: "SaveFormat"
    compression: "CompressionOptions" | None


class RequestState(TypedDict):
    """Fully-resolved runtime state (all keys present)."""

    endpoint: str | None
    method: str
    data: Any
    headers: dict[str, str] | None
    auth_type: Type["AuthBase"] | None
    jmespath_expression: str | None
    save_format: "SaveFormat"
    source_format: "SaveFormat"
    compression: "CompressionOptions" | None


class MultiHttpToFilesystem(HttpToFilesystem):
    """
    Execute multiple HTTP requests in a single task and save each response as a separate file.

    This operator extends HttpToFilesystem to process multiple requests efficiently
    while reducing Airflow task overhead. Each request can override base configuration
    (endpoint, method, headers, data, etc.) and results are saved as sequential files.

    Args:
        multi_requests: List of request specifications. Each item can override
                       any base operator parameter for that specific request.

    Example:
        MultiHttpToFilesystem(
            http_conn_id='api_connection',
            base_endpoint='/api/v1',
            headers={'Authorization': 'Bearer token'},
            multi_requests=[
                {'endpoint': '/users/1'},
                {'endpoint': '/users/2', 'method': 'POST', 'data': {...}},
                {'endpoint': '/orders', 'headers': {'Custom': 'Header'}}
            ]
        )

    Notes:
        - Pagination is not supported
        - Requests are executed sequentially within the task
        - Per-request values override base configuration with dict merging for
          headers/data, and replacement for other parameters
        - All validations are re-applied after each request configuration
    """

    template_fields = HttpToFilesystem.template_fields + ["multi_requests"]
    template_fields_renderers = {
        **HttpToFilesystem.template_fields_renderers,
        "multi_requests": "py",
    }

    # Allowed keys come from the TypedDict (so static + runtime stay in sync)
    _ALLOWED_KEYS = set(RequestSpec.__annotations__.keys())

    def __init__(self, *, multi_requests: list[RequestSpec], **kwargs):
        # No pagination in this multi operator
        if kwargs.get("pagination_function") is not None:
            raise ValueError("Pagination is not supported in MultiHttpToFilesystem")

        if not multi_requests or not isinstance(multi_requests, list):
            raise ValueError("multi_requests must be a non-empty list of RequestSpec")

        super().__init__(**kwargs)
        self.multi_requests: list[RequestSpec] = multi_requests

    def _capture_request_state(self) -> RequestState:
        return {
            "endpoint": self.endpoint,
            "method": self.method,
            "data": self.data,
            "headers": self.headers,
            "auth_type": self.auth_type,
            "jmespath_expression": self.jmespath_expression,
            "save_format": self.save_format,
            "source_format": self.source_format,
            "compression": self.compression,
        }

    def _restore_request_state(self, state: RequestState) -> None:
        self.endpoint = state["endpoint"]
        self.method = state["method"]
        self.data = state["data"]
        self.headers = state["headers"]
        self.auth_type = state["auth_type"]
        self.jmespath_expression = state["jmespath_expression"]
        self.save_format = state["save_format"]
        self.source_format = state["source_format"]
        self.compression = state["compression"]

    @staticmethod
    def _merge_or_replace(base_val: Any, override_val: Any) -> Any:
        # Shallow-merge dicts; otherwise replace.
        if isinstance(base_val, dict) and isinstance(override_val, dict):
            return {**base_val, **override_val}
        return override_val

    def _apply_request_overrides(self, spec: RequestSpec, base: RequestState) -> None:
        """
            Apply per-request configuration overrides on top of base operator settings.

        Args:
            spec: Request-specific configuration overrides
            base: Base operator configuration to restore after request

        Raises:
            ValueError: If spec contains unknown keys or invalid combinations
        """

        # Validate allowed override keys
        unknown = set(spec.keys()) - self._ALLOWED_KEYS
        if unknown:
            raise ValueError(f"Unknown keys in multi_requests item: {sorted(unknown)}")

        # Simple fields
        self.endpoint = spec.get("endpoint", base["endpoint"])
        self.method = spec.get("method", base["method"])
        self.auth_type = spec.get("auth_type", base["auth_type"])
        self.jmespath_expression = spec.get(
            "jmespath_expression", base["jmespath_expression"]
        )

        # Dict-like merge/replace behavior
        if "headers" in spec:
            self.headers = self._merge_or_replace(base["headers"], spec["headers"])
        else:
            self.headers = base["headers"]

        if "data" in spec:
            self.data = self._merge_or_replace(base["data"], spec["data"])
        else:
            self.data = base["data"]

        # Formats & compression
        self.save_format = spec.get("save_format", base["save_format"])
        self.source_format = spec.get("source_format", base["source_format"])
        self.compression = spec.get("compression", base["compression"])

        # Validate this request's final state
        self._validate_current_request_state()

    def _validate_current_request_state(self) -> None:
        # Re-apply critical validations that may be affected by per-request overrides
        if (
            self.save_format in self.binary_response_source_format
            and self.compression is not None
        ):
            raise ValueError(
                f"Compression is not supported for binary response save formats: "
                f"{self.binary_response_source_format}"
            )
        if self.data_transformation and not callable(self.data_transformation):
            raise ValueError("data_transformation must be a callable")
        if self.data_transformation is None and self.source_format != self.save_format:
            raise ValueError(
                "data_transformation must be provided if source_format differs from save_format"
            )
        if self.data_transformation_kwargs and self.data_transformation is None:
            raise ValueError(
                "data_transformation must be provided if data_transformation_kwargs is provided"
            )

    def execute(self, context) -> Any:
        base = self._capture_request_state()
        for i, spec in enumerate(self.multi_requests, start=1):
            self.file_number_start = i
            try:
                self._apply_request_overrides(spec, base)
                super().execute(context)
            finally:
                self._restore_request_state(base)


def list_to_jsonl(data: list[dict], compression: "CompressionOptions") -> BytesIO:
    out = StringIO()
    df = pd.DataFrame(data)
    df.to_json(out, orient="records", lines=True, compression=compression)
    out.seek(0)
    return BytesIO(out.getvalue().encode())


def json_to_binary(data: dict, compression: "CompressionOptions") -> BytesIO:
    json_string = json.dumps(data).encode()
    compressed_json = compress(compression, json_string)
    out = BytesIO(compressed_json)
    return out


def csv_to_binary(data: str, compression: "CompressionOptions") -> BytesIO:
    csv_df = pd.read_csv(StringIO(data), sep=",")
    out = BytesIO()
    csv_df.to_csv(out, compression=compression, index=False)
    out.seek(0)
    return out


def xml_to_binary(data: str, compression: "CompressionOptions") -> BytesIO:
    compressed_xml = compress(compression, data.encode())
    out = BytesIO(compressed_xml)
    return out
